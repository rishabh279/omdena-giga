{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Online installations</span>","metadata":{}},{"cell_type":"code","source":"# !pip install pycocotools>=2.0.2\n# !pip install timm>=0.3.2\n# !pip install omegaconf>=2.0\n# !pip install ensemble-boxes","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T13:13:59.221524Z","iopub.execute_input":"2022-01-19T13:13:59.221996Z","iopub.status.idle":"2022-01-19T13:13:59.227919Z","shell.execute_reply.started":"2022-01-19T13:13:59.221953Z","shell.execute_reply":"2022-01-19T13:13:59.22691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Offline installations</span>","metadata":{}},{"cell_type":"code","source":"## Install omegaconf - dependancy for effdet\n!pip install --no-deps ../input/effdet-latestvinbigdata-wbf-fused/omegaconf-2.0.6-py3-none-any.whl\n\n## Install timm & ensemble_boxes\n!pip install --no-deps ../input/effdet-latestvinbigdata-wbf-fused/timm-0.3.4-py3-none-any.whl\n!pip install --no-deps ../input/effdet-latestvinbigdata-wbf-fused/ensemble_boxefs-1.0.4-py3-none-any.whl\n\n!cp -r ../input/effdet-latestvinbigdata-wbf-fused/pycocotools-2.0.2/ .\n!cd ./pycocotools-2.0.2 && python setup.py install\n!rm -r pycocotools-2.0.2\n\n!cp -r ../input/effdet-latestvinbigdata-wbf-fused/efficientdet-pytorch/ .\n!cd ./efficientdet-pytorch && python setup.py install\n!rm -r efficientdet-pytorch","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-22T09:56:04.986960Z","iopub.execute_input":"2022-01-22T09:56:04.987418Z","iopub.status.idle":"2022-01-22T09:56:23.317010Z","shell.execute_reply.started":"2022-01-22T09:56:04.987368Z","shell.execute_reply":"2022-01-22T09:56:23.316019Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Release GPU Memory.\nfrom numba import cuda as CU\ntry:\n    device = CU.get_current_device()\n    device.reset()\nexcept Exception as E:\n    print(\"GPU not enabled. Nothing to clear and good to go.\")\n\n\n## Restart session to detect installed libraries\n!pip list | grep effdet\nimport os\nos._exit(00)\n## Check PyTorch Version","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:56:32.797867Z","iopub.execute_input":"2022-01-22T09:56:32.798225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Import Packages</span>","metadata":{}},{"cell_type":"code","source":"import sys\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport warnings\nfrom collections import Counter\n\n#from ensemble_boxes import weighted_boxes_fusion\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.utils.data.dataloader import default_collate\n\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain,DetBenchPredict\nfrom effdet.efficientdet import HeadNet\nfrom effdet import create_model, unwrap_bench, create_loader, create_dataset, create_evaluator, create_model_from_config\nfrom effdet.data import resolve_input_config, SkipSubset\nfrom effdet.anchors import Anchors, AnchorLabeler\nfrom timm.models import resume_checkpoint, load_checkpoint\nfrom timm.utils import *\nfrom timm.optim import create_optimizer\nfrom timm.scheduler import create_scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:04.117103Z","iopub.execute_input":"2022-01-22T09:57:04.117436Z","iopub.status.idle":"2022-01-22T09:57:07.255452Z","shell.execute_reply.started":"2022-01-22T09:57:04.117406Z","shell.execute_reply":"2022-01-22T09:57:07.254477Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Seed Everything</span>\n","metadata":{}},{"cell_type":"code","source":"SEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:08.997734Z","iopub.execute_input":"2022-01-22T09:57:08.998060Z","iopub.status.idle":"2022-01-22T09:57:09.010097Z","shell.execute_reply.started":"2022-01-22T09:57:08.998032Z","shell.execute_reply":"2022-01-22T09:57:09.008955Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = os.listdir('../input/originaldataset/original_dataset_split/original_dataset_split/train/school')\nval = os.listdir('../input/originaldataset/original_dataset_split/original_dataset_split/val/school')\ntest = os.listdir('../input/originaldataset/original_dataset_split/original_dataset_split/test/school')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:13.820694Z","iopub.execute_input":"2022-01-22T09:57:13.821058Z","iopub.status.idle":"2022-01-22T09:57:14.917512Z","shell.execute_reply.started":"2022-01-22T09:57:13.821029Z","shell.execute_reply":"2022-01-22T09:57:14.916614Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Load Data</span>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/schooldata/school_data/final_data.csv')\ndata.rename(columns={'filename':'image_id', 'x1':'x_min', 'y1':'y_min', 'x2':'x_max', 'y2':'y_max'}, inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:27.191035Z","iopub.execute_input":"2022-01-22T09:57:27.191364Z","iopub.status.idle":"2022-01-22T09:57:27.238992Z","shell.execute_reply.started":"2022-01-22T09:57:27.191334Z","shell.execute_reply":"2022-01-22T09:57:27.238162Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data['split'] = 'na'\ndata.loc[data[data['image_id'].isin(train)].index, 'split'] = 'train' \ndata.loc[data[data['image_id'].isin(val)].index, 'split'] = 'valid'\ndata.loc[data[data['image_id'].isin(test)].index, 'split']  = 'test'","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:29.805475Z","iopub.execute_input":"2022-01-22T09:57:29.805958Z","iopub.status.idle":"2022-01-22T09:57:29.825474Z","shell.execute_reply.started":"2022-01-22T09:57:29.805914Z","shell.execute_reply":"2022-01-22T09:57:29.824447Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data['split'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T08:55:41.926693Z","iopub.execute_input":"2022-01-22T08:55:41.927094Z","iopub.status.idle":"2022-01-22T08:55:41.937934Z","shell.execute_reply.started":"2022-01-22T08:55:41.927061Z","shell.execute_reply":"2022-01-22T08:55:41.936725Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#data['image_path'] = data['image_id'].map(lambda x:os.path.join('../input/school-data2/school_data/schools_annotated',str(x)))\ndata['class_id'] = [1] * data.shape[0]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:33.492453Z","iopub.execute_input":"2022-01-22T09:57:33.492853Z","iopub.status.idle":"2022-01-22T09:57:33.512206Z","shell.execute_reply.started":"2022-01-22T09:57:33.492820Z","shell.execute_reply":"2022-01-22T09:57:33.510856Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = data[~data['image_id'].isin(['42201308.png','61212325.png', '61223303.png'])]\ndata.drop_duplicates(subset=['image_id'], keep='first', inplace=True)\ndata.reset_index(drop=True, inplace=True)\ndata.drop(columns='id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:35.875672Z","iopub.execute_input":"2022-01-22T09:57:35.876036Z","iopub.status.idle":"2022-01-22T09:57:35.892160Z","shell.execute_reply.started":"2022-01-22T09:57:35.875985Z","shell.execute_reply":"2022-01-22T09:57:35.891362Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ns = []\nsecd = data[data['split']=='train']['image_id'].tolist()\nfor elem in train:\n    if elem not in secd:\n        train_ns.append(elem)\nval_ns = []\nsecd = data[data['split']=='valid']['image_id'].tolist()\nfor elem in val:\n    if elem not in secd:\n        val_ns.append(elem)\ntest_ns = []\nsecd = data[data['split']=='test']['image_id'].tolist()\nfor elem in test:\n    if elem not in secd:\n        test_ns.append(elem)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:38.170890Z","iopub.execute_input":"2022-01-22T09:57:38.171220Z","iopub.status.idle":"2022-01-22T09:57:38.612337Z","shell.execute_reply.started":"2022-01-22T09:57:38.171189Z","shell.execute_reply":"2022-01-22T09:57:38.611545Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"split_train = ['train'] * len(train_ns)\nsplit_val = ['valid'] * len(val_ns)\nsplit_test = ['test'] * len(test_ns)\nfinal_split = split_train + split_val + split_test\nfinal_ns = train_ns + val_ns + test_ns\nx_min = [0] * len(final_ns)\ny_min = [0] * len(final_ns)\nx_max = [256] * len(final_ns)\ny_max = [256] * len(final_ns)\nclass_id = [0] * len(final_ns)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:42.044920Z","iopub.execute_input":"2022-01-22T09:57:42.045256Z","iopub.status.idle":"2022-01-22T09:57:42.052612Z","shell.execute_reply.started":"2022-01-22T09:57:42.045224Z","shell.execute_reply":"2022-01-22T09:57:42.051593Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data2 = pd.DataFrame()\ndata2['x_min'] = x_min\ndata2['y_min'] = y_min\ndata2['x_max'] = x_max\ndata2['y_max'] = y_max\ndata2['image_id'] = final_ns\ndata2['split'] = final_split\ndata2['class_id'] = class_id\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:46.047409Z","iopub.execute_input":"2022-01-22T09:57:46.047922Z","iopub.status.idle":"2022-01-22T09:57:46.083424Z","shell.execute_reply.started":"2022-01-22T09:57:46.047883Z","shell.execute_reply":"2022-01-22T09:57:46.082093Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data2 = data2[~data2['image_id'].isin(['42304318_E.png', '41606305_N.png','11302316_E.png', '11301320_E.png', 'train.json', 'test.json', 'val.json', '23301340_W.png', '42201308.png'])]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:48.570072Z","iopub.execute_input":"2022-01-22T09:57:48.571675Z","iopub.status.idle":"2022-01-22T09:57:48.585006Z","shell.execute_reply.started":"2022-01-22T09:57:48.571624Z","shell.execute_reply":"2022-01-22T09:57:48.583582Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data, data2], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:50.791062Z","iopub.execute_input":"2022-01-22T09:57:50.791396Z","iopub.status.idle":"2022-01-22T09:57:50.798975Z","shell.execute_reply.started":"2022-01-22T09:57:50.791365Z","shell.execute_reply":"2022-01-22T09:57:50.797675Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data.shape\ndata['split'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:52.752929Z","iopub.execute_input":"2022-01-22T09:57:52.753264Z","iopub.status.idle":"2022-01-22T09:57:52.763538Z","shell.execute_reply.started":"2022-01-22T09:57:52.753228Z","shell.execute_reply":"2022-01-22T09:57:52.762458Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data = data[~data['image_id'].isin(['61212325.png', '61223303.png'])]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:55.187909Z","iopub.execute_input":"2022-01-22T09:57:55.188238Z","iopub.status.idle":"2022-01-22T09:57:55.196767Z","shell.execute_reply.started":"2022-01-22T09:57:55.188207Z","shell.execute_reply":"2022-01-22T09:57:55.195754Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Augmentations & Transforms</span>\n\nUsign the Albumentations package to create an augmentation pipeline for training and validation. ","metadata":{}},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose(\n        [\n        A.Resize(height=512, width=512, p=1),\n        ToTensorV2(p=1.0)\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_test_transform():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0)], \n            p=1.0, \n            bbox_params=A.BboxParams(\n                format='pascal_voc',\n                min_area=0, \n                min_visibility=0,\n                label_fields=['labels']\n            ))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:57:57.649712Z","iopub.execute_input":"2022-01-22T09:57:57.650037Z","iopub.status.idle":"2022-01-22T09:57:57.660771Z","shell.execute_reply.started":"2022-01-22T09:57:57.650007Z","shell.execute_reply":"2022-01-22T09:57:57.659828Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Dataset Retrieval and Pre-processsing</span>\n\nDefine the data loader class to retrive images, perform albumentation-based + custom CutMix and MixUp augmentations","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, TRAIN_ROOT_PATH, transforms=None, test=False):\n        super().__init__()\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n        self.TRAIN_ROOT_PATH = TRAIN_ROOT_PATH\n        \n        \n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        #print(image_id)\n        image = cv2.imread(f'{self.TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        records = self.marking[self.marking['image_id'] == image_id]\n        boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n        labels = records['class_id'].tolist()\n        resize_transform = A.Compose([A.Resize(height=512, width=512, p=1.0)], \n                                    p=1.0, \n                                    bbox_params=A.BboxParams(\n                                        format='pascal_voc',\n                                        min_area=0.1, \n                                        min_visibility=0.1,\n                                        label_fields=['labels'])\n                                    )\n\n        resized = resize_transform(**{\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            })\n\n        resized_bboxes = np.vstack((list(bx) for bx in resized['bboxes']))\n        return resized['image'], resized_bboxes, resized['labels']\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n    \n        image, boxes, labels = self.load_image_and_boxes(index)\n        ## To prevent ValueError: y_max is less than or equal to y_min for bbox from albumentations bbox_utils\n        labels = np.array(labels, dtype=np.int).reshape(len(labels), 1)\n        combined = np.hstack((boxes.astype(np.int), labels))\n        combined = combined[np.logical_and(combined[:,2] > combined[:,0],\n                                                          combined[:,3] > combined[:,1])]\n        boxes = combined[:, :4]\n        labels = combined[:, 4].tolist()\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  ## ymin, xmin, ymax, xmax\n                    break\n            \n            ## Handling case where no valid bboxes are present\n            if len(target['boxes'])==0 or i==9:\n                return None\n            else:\n                ## Handling case where augmentation and tensor conversion yields no valid annotations\n                try:\n                    assert torch.is_tensor(image), f\"Invalid image type:{type(image)}\"\n                    assert torch.is_tensor(target['boxes']), f\"Invalid target type:{type(target['boxes'])}\"\n                except Exception as E:\n                    print(\"Image skipped:\", E)\n                    return None      \n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:00.381841Z","iopub.execute_input":"2022-01-22T09:58:00.382169Z","iopub.status.idle":"2022-01-22T09:58:00.411697Z","shell.execute_reply.started":"2022-01-22T09:58:00.382139Z","shell.execute_reply":"2022-01-22T09:58:00.410561Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# sample = data[data['split']=='valid']\n\n# for i in range(sample.shape[0]):\n#     #try:\n#     DatasetRetriever(sample, sample['image_id'].tolist(), '../input/originaldataset/original_dataset_split/original_dataset_split/val/school')[i]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-21T22:20:48.303798Z","iopub.execute_input":"2022-01-21T22:20:48.304202Z","iopub.status.idle":"2022-01-21T22:20:48.308599Z","shell.execute_reply.started":"2022-01-21T22:20:48.304164Z","shell.execute_reply":"2022-01-21T22:20:48.3075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Helper Functions</span>\n\nFunction to store Average and Current values","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:04.681337Z","iopub.execute_input":"2022-01-22T09:58:04.681732Z","iopub.status.idle":"2022-01-22T09:58:04.688936Z","shell.execute_reply.started":"2022-01-22T09:58:04.681700Z","shell.execute_reply":"2022-01-22T09:58:04.687921Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Build Training and Validation Loops</span>","metadata":{}},{"cell_type":"code","source":"class Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        \n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = config.OptimizerClass(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        history_dict = {}\n        history_dict['epoch'] = []\n        history_dict['train_loss'] = []\n        history_dict['val_loss'] = []\n        history_dict['train_lr'] = []\n        \n        for e in range(self.config.n_epochs):\n            history_dict['epoch'].append(self.epoch)\n            lr = self.optimizer.param_groups[0]['lr']\n            timestamp = datetime.utcnow().isoformat()\n            \n            if self.config.verbose:\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, loss_trend, lr_trend = self.train_epoch(train_loader)\n            history_dict['train_loss'].append(loss_trend)\n            history_dict['train_lr'].append(lr_trend)\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n            \n            t = time.time()\n            summary_loss, loss_trend = self.validation(validation_loader)\n            history_dict['val_loss'].append(loss_trend)\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            \n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                \n                try:\n                    os.remove(f)\n                except:pass\n                f = f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin'\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n        return history_dict\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        loss_trend = []\n        lr_trend = []\n        for step, (images, targets, image_ids) in tqdm(enumerate(train_loader), total=len(train_loader)):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )            \n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            target_res = {}\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n            target_res['bbox'] = boxes\n            target_res['cls'] = labels\n            self.optimizer.zero_grad()\n            output = self.model(images, target_res)\n\n            loss = output['loss']\n            loss.backward()\n            summary_loss.update(loss.detach().item(), self.config.batch_size)\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n            \n            lr = self.optimizer.param_groups[0]['lr']\n            loss_trend.append(summary_loss.avg)\n            lr_trend.append(lr)\n        return summary_loss, loss_trend, lr_trend\n    \n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        loss_trend = []\n#         lr_trend = []\n        \n        for step, (images, targets, image_ids) in tqdm(enumerate(val_loader), total=len(val_loader)):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                images = images.to(self.device).float()\n                target_res = {}\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n                target_res['bbox'] = boxes\n                target_res['cls'] = labels \n                target_res[\"img_scale\"] = torch.tensor([1.0] * self.config.batch_size,\n                                                       dtype=torch.float).to(self.device)\n                target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * self.config.batch_size,\n                                                      dtype=torch.float).to(self.device)\n                \n                output = self.model(images, target_res)\n        \n                loss = output['loss']\n                summary_loss.update(loss.detach().item(), self.config.batch_size)\n\n                loss_trend.append(summary_loss.avg)\n        return summary_loss, loss_trend[-1]\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:12.993875Z","iopub.execute_input":"2022-01-22T09:58:12.994216Z","iopub.status.idle":"2022-01-22T09:58:13.042251Z","shell.execute_reply.started":"2022-01-22T09:58:12.994185Z","shell.execute_reply":"2022-01-22T09:58:13.041261Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Define Training Configuration</span>","metadata":{}},{"cell_type":"code","source":"class TrainGlobalConfig:\n    def __init__(self):\n        self.num_classes = 2\n        self.num_workers = 4\n        self.batch_size = 4 \n        self.n_epochs = 10\n        self.lr = 0.0002\n        self.model_name = 'tf_efficientdet_d1'\n        self.folder = 'training_job'\n        self.verbose = True\n        self.verbose_step = 1\n        self.step_scheduler = True\n        self.validation_scheduler = False\n        self.n_img_count = len(data.image_id.unique())\n        self.OptimizerClass = torch.optim.AdamW\n        self.SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n        self.scheduler_params = dict(\n                            T_0=50,\n                            T_mult=1,\n                            eta_min=0.0001,\n                            last_epoch=-1,\n                            verbose=False\n                            )\n        self.kfold = 2\n    \n    def reset(self):\n        self.OptimizerClass = torch.optim.AdamW\n        self.SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n\ntrain_config = TrainGlobalConfig()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:20.830699Z","iopub.execute_input":"2022-01-22T09:58:20.831181Z","iopub.status.idle":"2022-01-22T09:58:20.842900Z","shell.execute_reply.started":"2022-01-22T09:58:20.831121Z","shell.execute_reply":"2022-01-22T09:58:20.841853Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Initiate Training</span>","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:24.900929Z","iopub.execute_input":"2022-01-22T09:58:24.901302Z","iopub.status.idle":"2022-01-22T09:58:24.907656Z","shell.execute_reply.started":"2022-01-22T09:58:24.901269Z","shell.execute_reply":"2022-01-22T09:58:24.906623Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = None\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n\ntrain_ids = data[data['split'] == 'train'].image_id.unique()\nval_ids = data[data['split'] == 'valid'].image_id.unique()\n\n\n# train_ids = train_ids[:10]\n# val_ids = val_ids[:10]\ntrain_dataset = DatasetRetriever(\n                    image_ids=train_ids,\n                    marking=data,\n                    TRAIN_ROOT_PATH='../input/originaldataset/original_dataset_split/original_dataset_split/train/school',\n                    transforms=get_train_transforms(),\n                    test=False,\n                    )\n\nvalidation_dataset = DatasetRetriever(\n                        image_ids=val_ids,\n                        marking=data,\n                        TRAIN_ROOT_PATH='../input/originaldataset/original_dataset_split/original_dataset_split/val/school',\n                        transforms=get_valid_transforms(),\n                        test=True,\n                        )\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=train_config.batch_size,\n    sampler=RandomSampler(train_dataset),\n    pin_memory=False,\n    drop_last=True,\n    num_workers=train_config.num_workers,\n    collate_fn=collate_fn,\n)\nval_loader = torch.utils.data.DataLoader(\n    validation_dataset, \n    batch_size=train_config.batch_size,\n    num_workers=train_config.num_workers,\n    shuffle=False,\n    sampler=SequentialSampler(validation_dataset),\n    pin_memory=False,\n    collate_fn=collate_fn,\n)\n    \n\nif(checkpoint_path):\n    print(f'Resuming from checkpoint: {checkpoint_path}')        \n    model = create_model_from_config(base_config, bench_task='train', bench_labeler=True,\n                             num_classes=train_config.num_classes,\n                             pretrained=False)\n    model.to(device)\n\n    fitter = Fitter(model=model, device=device, config=train_config)\n    fitter.load(checkpoint_path)\n\nelse:\n    \n    config = get_efficientdet_config('tf_efficientdet_d1')\n    config.num_classes = 1\n    config.image_size = (512, 512)\n    net = EfficientDet(config, pretrained_backbone=True)\n    net.class_net = HeadNet(\n        config,\n        num_outputs=config.num_classes,\n    )\n    model = DetBenchTrain(net, config)\n    model.to(device)\n\n    fitter = Fitter(model=model, device=device, config=train_config)  \n        \n    model_config = model.config\n    history_dict = fitter.fit(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T08:56:31.197846Z","iopub.execute_input":"2022-01-22T08:56:31.198234Z","iopub.status.idle":"2022-01-22T08:56:50.037353Z","shell.execute_reply.started":"2022-01-22T08:56:31.198200Z","shell.execute_reply":"2022-01-22T08:56:50.035901Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Inference","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data['split'] == 'test')&(data['class_id']==0)].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:58:36.894833Z","iopub.execute_input":"2022-01-22T09:58:36.895163Z","iopub.status.idle":"2022-01-22T09:58:36.931901Z","shell.execute_reply.started":"2022-01-22T09:58:36.895132Z","shell.execute_reply":"2022-01-22T09:58:36.931070Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"len(test_ids)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:08:26.531661Z","iopub.execute_input":"2022-01-22T10:08:26.532035Z","iopub.status.idle":"2022-01-22T10:08:26.539052Z","shell.execute_reply.started":"2022-01-22T10:08:26.532003Z","shell.execute_reply":"2022-01-22T10:08:26.537636Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#test_ids = data[(data['split'] == 'test')&(data['class_id']==0)].image_id.unique()\ntest_ids = data[(data['split'] == 'test')].image_id.unique()\ntest_dataset = DatasetRetriever(\n                            image_ids=test_ids,\n                            marking=data,\n                            TRAIN_ROOT_PATH='../input/originaldataset/original_dataset_split/original_dataset_split/test/school',\n                            transforms=get_test_transform(),\n                            test=True,\n                            )\ntest_loader = DataLoader(test_dataset,\n                         batch_size = 4,\n                         shuffle = False,\n                         drop_last = False,\n                         collate_fn = collate_fn) ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:08:23.231889Z","iopub.execute_input":"2022-01-22T10:08:23.232237Z","iopub.status.idle":"2022-01-22T10:08:23.242787Z","shell.execute_reply.started":"2022-01-22T10:08:23.232208Z","shell.execute_reply":"2022-01-22T10:08:23.241790Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# valid_ids = data[data['split'] == 'valid'].image_id.unique()\n# test_dataset = DatasetRetriever(\n#                             image_ids=valid_ids,\n#                             marking=data,\n#                             TRAIN_ROOT_PATH='../input/originaldataset/original_dataset_split/original_dataset_split/val/school',\n#                             transforms=get_valid_transforms(),\n#                             test=True,\n#                             )\n# test_loader = DataLoader(test_dataset,\n#                          batch_size = 4,\n#                          shuffle = False,\n#                          drop_last = False,\n#                          collate_fn = collate_fn) ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:45:24.346126Z","iopub.execute_input":"2022-01-22T10:45:24.346465Z","iopub.status.idle":"2022-01-22T10:45:24.358337Z","shell.execute_reply.started":"2022-01-22T10:45:24.346433Z","shell.execute_reply":"2022-01-22T10:45:24.357449Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/alexhock/object-detection-metrics","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:59:31.206846Z","iopub.execute_input":"2022-01-22T09:59:31.207171Z","iopub.status.idle":"2022-01-22T09:59:40.811670Z","shell.execute_reply.started":"2022-01-22T09:59:31.207141Z","shell.execute_reply":"2022-01-22T09:59:40.810775Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!pip install ensemble-boxes","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:59:40.815041Z","iopub.execute_input":"2022-01-22T09:59:40.815386Z","iopub.status.idle":"2022-01-22T09:59:47.910626Z","shell.execute_reply.started":"2022-01-22T09:59:40.815353Z","shell.execute_reply":"2022-01-22T09:59:47.909756Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from ensemble_boxes import ensemble_boxes_wbf\ndef run_wbf(predictions, image_size=512, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n    bboxes = []\n    confidences = []\n    class_labels = []\n\n    for prediction in predictions:\n        boxes = [(prediction[\"boxes\"] / image_size).tolist()]\n        scores = [prediction[\"scores\"].tolist()]\n        labels = [prediction[\"classes\"].tolist()]\n\n        boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(\n            boxes,\n            scores,\n            labels,\n            weights=weights,\n            iou_thr=iou_thr,\n            skip_box_thr=skip_box_thr,\n        )\n        boxes = boxes * (image_size - 1)\n        bboxes.append(boxes.tolist())\n        confidences.append(scores.tolist())\n        class_labels.append(labels.tolist())\n    return bboxes, confidences, class_labels\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:00:38.674523Z","iopub.execute_input":"2022-01-22T10:00:38.674915Z","iopub.status.idle":"2022-01-22T10:00:39.023090Z","shell.execute_reply.started":"2022-01-22T10:00:38.674881Z","shell.execute_reply":"2022-01-22T10:00:39.022258Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from objdetecteval.metrics.coco_metrics import get_coco_stats\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\nconfig = get_efficientdet_config('tf_efficientdet_d1')\nconfig.num_classes = 1\nconfig.image_size = (512, 512)\nnet = EfficientDet(config, pretrained_backbone=True)\nnet.class_net = HeadNet(\n    config,\n    num_outputs=config.num_classes,\n)\n\ncheckpoint = torch.load('../input/checkpoints/best-checkpoint-004epoch.bin')\nnet.load_state_dict(checkpoint['model_state_dict'])\n\ndel checkpoint\ngc.collect()\nnet = DetBenchPredict(net)\nnet.eval()\nnet = net.cuda()\nresult = []\ntruth_image_ids = []\ntruth_boxes = []\ntruth_labels = []\npredicted_labels = []\npredicted_boxes = []\npredicted_confidences = []\nimage_ids = []\nfor step, (images, targets, image_ids) in tqdm(enumerate(test_loader), total=len(test_loader)):\n    with torch.no_grad():\n        images = torch.stack(images)\n        images = images.to(device).float()\n        target_res = {}\n\n        boxes = [target['boxes'][:, [1, 0, 3, 2]].detach().tolist() for target in targets]\n        labels = [target['labels'].detach().tolist() for target in targets]\n        \n        target_res[\"img_scale\"] = torch.tensor([1.0] * 4,\n                                               dtype=torch.float).to(device)\n        target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * 4,\n                                              dtype=torch.float).to(device)\n        truth_boxes.extend(boxes)\n        truth_labels.extend(labels)\n        \n        output = net(images, {'img_scale':target_res[\"img_scale\"], 'img_size':target_res[\"img_size\"]})\n        for i in range(images.shape[0]):\n            boxes = output[i].detach().cpu().numpy()[:,:4]\n            scores = output[i].detach().cpu().numpy()[:,4]\n            classes = output[i].detach().cpu().numpy()[:, 5]\n            \n            indexes = np.where(scores > 0.4)[0]\n            result.append({\n                'boxes': boxes[indexes],\n                'scores': scores[indexes],\n                'classes': classes[indexes],\n            })\n            #print(scores)\n            boxes, scores, labels = run_wbf(result)\n#             print(boxes)\n#             print(scores)\n#             print(labels)\n            if not boxes[0]:\n                boxes = [[[0, 0, 512, 512]]]\n                scores = [[1]]\n                labels = [[0]]\n#                 print(boxes)\n#                 print(scores)\n#                 print(labels)\n            result = []\n            predicted_boxes.extend(boxes)\n            predicted_labels.extend(labels)\n            predicted_confidences.extend(scores)\n            truth_image_ids.append(image_ids[i])\n            \nstats = get_coco_stats(\n        prediction_image_ids=truth_image_ids,\n        predicted_class_confidences=predicted_confidences,\n        predicted_bboxes=predicted_boxes,\n        predicted_class_labels=predicted_labels,\n        target_image_ids=truth_image_ids,\n        target_bboxes=truth_boxes,\n        target_class_labels=truth_labels,\n    )['All']","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:45:35.631972Z","iopub.execute_input":"2022-01-22T10:45:35.632389Z","iopub.status.idle":"2022-01-22T10:46:14.198962Z","shell.execute_reply.started":"2022-01-22T10:45:35.632349Z","shell.execute_reply":"2022-01-22T10:46:14.198181Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"a, b = [], []\nfor image_id, labels, boxes, confidences in zip(truth_image_ids, predicted_labels, predicted_boxes, predicted_confidences):\n    if len(labels) > 0:\n        if len(labels) > 1:\n            for lab, bx, cf in zip(labels, boxes, confidences):\n                a.append(image_id)\n                a.append(lab)\n                a.append(bx)\n                a.append(cf)\n                b.append(a)\n                a = []\n        else:\n            a.append(image_id)\n            a.append(labels[0])\n            a.append(boxes[0])\n            a.append(confidences[0])\n            b.append(a)\n            a = []\n    else:\n        a.append(image_id)\n        a.append(0)\n        a.append([0, 0, 0, 0])\n        a.append(0)\n        b.append(a)\n        a=[]\n        \npredicted_df = pd.DataFrame(b, columns=['image_name', 'label', 'box', 'score'])\npredicted_df[['xmin', 'ymin', 'xmax', 'ymax']] = pd.DataFrame(predicted_df['box'].to_list(), index=predicted_df.index)\npredicted_df.drop(columns='box', inplace=True)\npredicted_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:46:38.655359Z","iopub.execute_input":"2022-01-22T10:46:38.655711Z","iopub.status.idle":"2022-01-22T10:46:38.691801Z","shell.execute_reply.started":"2022-01-22T10:46:38.655679Z","shell.execute_reply":"2022-01-22T10:46:38.690955Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"a, b = [], []\nfor image_id, labels, boxes in zip(truth_image_ids, truth_labels, truth_boxes):\n    if len(labels) > 0:\n        if len(labels) > 1:\n            for lab, bx in zip(labels, boxes):\n                a.append(image_id)\n                a.append(lab)\n                a.append(bx)\n                b.append(a)\n                a = []\n        else:\n            a.append(image_id)\n            a.append(labels[0])\n            a.append(boxes[0])\n            b.append(a)\n            a = []\n#     else:\n#         a.append(image_id)\n#         a.append(0)\n#         a.append([0, 0, 0, 0])\n#         b.append(a)\n#         a=[]\n        \ntruth_df = pd.DataFrame(b, columns=['image_name', 'label', 'box'])\ntruth_df[['xmin', 'ymin', 'xmax', 'ymax']] = pd.DataFrame(truth_df['box'].to_list(), index=truth_df.index)\ntruth_df.drop(columns='box', inplace=True)\ntruth_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:46:43.545876Z","iopub.execute_input":"2022-01-22T10:46:43.546195Z","iopub.status.idle":"2022-01-22T10:46:43.576144Z","shell.execute_reply.started":"2022-01-22T10:46:43.546166Z","shell.execute_reply":"2022-01-22T10:46:43.575431Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from objdetecteval.metrics.iou import iou\nimport pandas as pd\nfrom typing import List\n\n__all__ = [\n    \"get_inference_metrics\",\n    \"summarise_inference_metrics\",\n    \"match_preds_to_targets\",\n    \"get_inference_metrics_from_df\"\n]\n\n\ndef get_inference_metrics_from_df(predictions_df, labels_df):\n\n    matched_bounding_boxes = match_preds_to_targets(\n        predictions_df, labels_df\n    )\n\n    return get_inference_metrics(**matched_bounding_boxes)\n\n\ndef get_unique_image_names(predictions_df, labels_df):\n    # get unique image names from both preds and labels\n    # need from both to capture images where there were no predictions\n    # and images where there were predictions but no labels\n    unique_preds_images = predictions_df['image_name'].unique().tolist()\n    unique_label_images = labels_df['image_name'].unique().tolist()\n    unique_images = sorted(list(set([*unique_preds_images, *unique_label_images])))\n    return unique_images\n\n\ndef match_preds_to_targets(predictions_df, labels_df):\n\n    # check for required df columns\n    pred_required_columns = ['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'label', 'score']\n    assert all(col in predictions_df.columns for col in pred_required_columns), \\\n        f\"missing or different column names - should be: {pred_required_columns}\"\n    label_required_columns = ['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'label']\n    assert all(col in labels_df.columns for col in label_required_columns), \\\n        f\"missing or diferent column names - should be {label_required_columns}\"\n\n    image_names = []\n    predicted_class_labels = []\n    predicted_bboxes = []\n    predicted_class_confidences = []\n    target_class_labels = []\n    target_bboxes = []\n    image_preds = {}\n\n    unique_images = get_unique_image_names(predictions_df, labels_df)\n\n    # index the dataframes by the image_name\n    preds_df_indexed = predictions_df.set_index('image_name')\n    labels_df_indexed = labels_df.set_index('image_name')\n\n    # loop through individual images\n    for image_name in unique_images:\n\n        # get the predictions and labels for each image\n        preds = preds_df_indexed.loc[image_name]\n        labels = labels_df_indexed.loc[image_name]\n\n        # create lists for all the bounding boxes, labels and scores\n        # for the image, pascal boxes\n        # [[xmin, ymin, xmax, ymax], []]\n        # [label, label]\n        # [score, score]\n        pred_image_bboxes = preds[['xmin', 'ymin', 'xmax', 'ymax']].values.tolist()\n        pred_image_class_labels = preds[['label']].values.tolist()\n        pred_image_class_confs = preds[['score']].values.tolist()\n\n        # add the predictions lists for the image\n        image_names.append(image_name)\n        predicted_class_labels.append(pred_image_class_labels)\n        predicted_class_confidences.append(pred_image_class_confs)\n        if isinstance(pred_image_bboxes[0], list):\n          predicted_bboxes.append(pred_image_bboxes)\n        else:\n          predicted_bboxes.append([pred_image_bboxes])\n        # create lists of the label bboxes and classes\n        labels_image_bboxes = labels[['xmin', 'ymin', 'xmax', 'ymax']].values.tolist()\n        labels_image_class_labels = labels[['label']].values.tolist()\n\n        # add the label lists for the image\n        target_class_labels.append(labels_image_class_labels)\n        if isinstance(labels_image_bboxes[0], list):\n          target_bboxes.append(labels_image_bboxes)\n        else:\n          target_bboxes.append([labels_image_bboxes])\n        \n\n    return {\n        \"image_ids\": image_names,\n        \"predicted_class_labels\": predicted_class_labels,\n        \"predicted_bboxes\": predicted_bboxes,\n        \"predicted_class_confidences\": predicted_class_confidences,\n        \"target_class_labels\": target_class_labels,\n        \"target_bboxes\": target_bboxes\n    }\n\n\n\ndef calc_iou(pred_bbox, true_bboxes):\n    iou_val = 0.0\n    for true_bbox in true_bboxes:\n        # assumes pascal\n        box_iou_val = iou(pred_bbox, true_bbox)\n        if box_iou_val > iou_val:\n            iou_val = box_iou_val\n    return iou_val\n\n\ndef calculate_detections(\n    all_image_ids,\n    all_pred_classes,\n    all_pred_bboxes,\n    all_pred_confs,\n    all_true_classes,\n    all_true_bboxes,\n    do_iou_calc=True,\n):\n    assert len(all_image_ids) == len(all_pred_bboxes) == len(all_true_bboxes)\n\n    # [\"image_id\", \"class\", \"TP\", \"TN\", \"FP\", \"FN\", \"Confidence\", \"IoU\"]\n    detections = []\n\n    for image_id, pred_classes, pred_boxes, pred_confs, true_classes, true_boxes in zip(\n        all_image_ids,\n        all_pred_classes,\n        all_pred_bboxes,\n        all_pred_confs,\n        all_true_classes,\n        all_true_bboxes,\n    ):\n        # loop through the predicted boxes for the image\n        for pred_class, pred_box, pred_conf in zip(\n            pred_classes, pred_boxes, pred_confs\n        ):\n            if type(pred_class) == list:\n              pred_class = pred_class[0]\n              pred_conf = pred_conf[0]\n            if pred_class in true_classes:\n                if do_iou_calc:\n                    box_iou = calc_iou(pred_box, true_boxes)\n                    detections.append(\n                        [image_id, pred_class, 1, 0, 0, 0, pred_conf, box_iou]\n                    )\n                else:\n                    # true positive\n                    detections.append([image_id, pred_class, 1, 0, 0, 0, pred_conf, -1])\n\n                continue\n\n            if pred_class not in true_classes:\n                # false positive\n                detections.append([image_id, pred_class, 0, 0, 1, 0, pred_conf, -1])\n                continue\n\n        # false negatives\n        for true_class in true_classes:\n            if true_class not in pred_classes:\n                detections.append([image_id, true_class, 0, 0, 0, 1, 0, -1])\n\n    return detections\n\n\ndef summarise_inference_metrics(inference_df):\n\n    class_stats = inference_df.groupby(\"class\")[[\"TP\", \"FP\", \"FN\"]].sum()\n\n    # total number for each class\n    class_stats[\"Total\"] = class_stats[[\"TP\", \"FP\", \"FN\"]].sum(axis=1)\n\n    class_stats[\"Precision\"] = class_stats[\"TP\"] / (\n        class_stats[\"TP\"] + class_stats[\"FP\"]\n    )\n    class_stats[\"Recall\"] = class_stats[\"TP\"] / (class_stats[\"TP\"] + class_stats[\"FN\"])\n\n    # remove the index creatd by the groupby so the class is a column\n    class_stats = class_stats.reset_index()\n\n    return class_stats\n\n\ndef get_inference_metrics(\n    image_ids: List[int],\n    predicted_class_labels: List[List[int]],\n    predicted_bboxes: List[List[List[float]]],\n    predicted_class_confidences: List[List[float]],\n    target_class_labels: List[List[int]],\n    target_bboxes: List[List[List[float]]],\n):\n    \"\"\"\n    Create metrics that do not include IoU. IoU is calculated but is not used to calculate precision and recall.\n    Converts the outputs from the models into inference dataframes containing evaluation metrics such as\n    precision and recall, and TP, FP, FN, confidence. Useful for more detailed analysis of results and plotting.\n    :param image_ids: A list of image ids for each image in the order of the prediction and target lists\n    :param predicted_class_labels: A list containing a list of class labels predicted per image\n    :param predicted_class_confidences: A list containing a list of prediction confidence values per image\n    :param predicted_bboxes: A list containing a list of bounding boxes, in Pascal VOC format, predicted per image\n    :param target_class_labels: A list containing a list of ground truth class labels per image\n    :param target_bboxes: A list containing a list of ground truth bounding boxes, in Pascal VOC format\n    :param conv_bbox_func: A function to convert the format of incoming bboxes to pascal format, default is None\n    :returns: a DataFrame of the results, and a dataframe containing precision and recall.\n    \"\"\"\n\n    detections = calculate_detections(\n        image_ids,\n        predicted_class_labels,\n        predicted_bboxes,\n        predicted_class_confidences,\n        target_class_labels,\n        target_bboxes,\n    )\n\n    inference_df = pd.DataFrame(\n        detections,\n        columns=[\"image_id\", \"class\", \"TP\", \"TN\", \"FP\", \"FN\", \"Confidence\", \"IoU\"],\n    )\n\n    return inference_df","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:32:28.366464Z","iopub.execute_input":"2022-01-22T10:32:28.366848Z","iopub.status.idle":"2022-01-22T10:32:28.406426Z","shell.execute_reply.started":"2022-01-22T10:32:28.366816Z","shell.execute_reply":"2022-01-22T10:32:28.405483Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"from objdetecteval.metrics import (\n    image_metrics as im,\n    coco_metrics as cm\n)\ninfer_df = get_inference_metrics_from_df(predicted_df, truth_df)\ninfer_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:46:55.715030Z","iopub.execute_input":"2022-01-22T10:46:55.715354Z","iopub.status.idle":"2022-01-22T10:46:58.622251Z","shell.execute_reply.started":"2022-01-22T10:46:55.715323Z","shell.execute_reply":"2022-01-22T10:46:58.621574Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class_summary_df = im.summarise_inference_metrics(infer_df)\nclass_summary_df","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:47:01.823033Z","iopub.execute_input":"2022-01-22T10:47:01.823378Z","iopub.status.idle":"2022-01-22T10:47:01.847348Z","shell.execute_reply.started":"2022-01-22T10:47:01.823347Z","shell.execute_reply":"2022-01-22T10:47:01.846592Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n#predictions = make_predictions(images)\n\n#i = 0\nsample = test_dataset[0][0].permute(1,2,0).cpu().numpy()\n\nboxes, scores, labels = run_wbf(result, image_index=0)\nboxes = boxes.astype(np.int32).clip(min=0, max=511)\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 1)\n    \nax.set_axis_off()\nax.imshow(sample);","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}